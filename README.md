# Llama2_Fine_Tuning
This Repo explain how to fine-tune Llama-2 using new techniques to overcome memory and computing limitations to make open-source large language models more accessible.

Fine-tuning Llama 2 using LoRA (Low-Rank Adaptation) and bitsandbytes involves leveraging these techniques to efficiently adapt the model to specific tasks with limited computational resources. Hereâ€™s a step-by-step guide on how you can approach this:

Prerequisites
Python: Make sure you have Python installed.
Hugging Face Transformers: Install the Hugging Face Transformers library.
Bitsandbytes: For efficient mixed-precision training.
LoRA: For efficient parameter adaptation.
